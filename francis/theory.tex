%!TEX root = francis_thesis.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Chapter about theory}\label{ch:THEORY}
\section{Computer Vision Tasks}
With the advance of computer vision, task in computer vision has moved from simple tasks  of image classification to complex task like semantic and instance segmentation. 
\subsection{Image Classification}
Image classification involves the assigning of land cover classes to pixels. Image classification refers
to the task of extracting information classes from a multi-band raster image. The raster produced from image classification can be used to create thematic maps. The purpose of the classification process is to categorize all pixels in a digital image into one of several land cover classes, or \textit{"themes"}.Depending on the interaction between the analyst and the computer during classification process, there are two types of classification, they are: supervised and unsupervised. In order to improve the classification accuracy, scientists have laid path in developing the advanced classification techniques. The aim of image classification is to identify and portray, as a unique gray level (or color), the features occurring in an image in terms of the object or type of land cover these features actually represent on the ground. 

Image classification analyzes the numerical properties of various image features and organizes data into categories. Classification algorithms usually use two phases of processing: training and testing. In the initial training phase, characteristic properties of typical image features are isolated and, based on these, a unique description of each classification category, i.e. training class, is created. In the successive testing phase, these feature-space segregation are used to classify image features. The description of training classes is an critical part of the classification process. In supervised classification, statistical processes (i.e. based on an a prior knowledge of probability distribution functions) or distribution-free processes can be utilized to extract class descriptors. Unsupervised classification depends on clustering algorithms to automatically group the training data into prototype classes. In either case, the motivating benchmark for constructing training classes are that they are:

\begin{enumerate}
\item Independent, e.a change in the description of one training class should not change the value of another,
\item Discriminatory, e.different image features should have significantly different descriptions, and
\item Reliable, all image features within a training group should share the common definitive descriptions of that group.
\end{enumerate}

 This representation allows us to consider each image feature as occupying a point, and each training class as occupying a sub-space (i.e. a representative point surrounded by some spread, or deviation), within the n-dimensional classification space. Viewed as such, the classification problem is that of determining to which sub-space class each feature vector belongs.
\begin{figure}[H]
  \centering
  \includegraphics[height=2in]{images/classification_detection_segmentaion_comparisons.jpeg}
   \caption{Image Classification,Object detection Semantic Segmentation. Source:towardsdatascience.com}
\end{figure}
\subsection{Object Detection}
 The objective of object detection is to identify all cases of objects from a referred to class, for example, individuals, vehicles or faces in a picture. Normally just a few occurrences of the item are available in the picture, yet there is an enormous number of potential locations and scales at which they can occur and that need to by one way or another be investigated. 
 Every location is accounted for with some type of pose information. This could be as straightforward as the area of the item, an area, and scale, or the degree of the object defined in terms of a bounding box. In different circumstances, the pose information information is more all-inclusive and has the parameters of a linear or non-linear transformation. For instance, a face identifier may process the locations of the eyes, nose, and mouth, in addition to the bounding box of the face. A case of a vehicle and individual detection that specifies the locations of certain parts is shown in Figure \ref{fig:objectD}. The pose could likewise be characterized by a three-dimensional dimensional transformation specifying the location of the object relative to the camera. 
Object detection systems construct a model for an object class from a set of training examples. In the case of a
fixed rigid object only one example may be needed, but more generally multiple training examples are
necessary to capture certain aspects of class variability.


\begin{figure}[H]
  \centering
  \includegraphics[height=3in]{images/object_det.jpeg}
   \caption{Object detection with bounding boxes. Source:steemit.com}
   \label{fig:objectD}
\end{figure}

Object detection strategies fall into two noteworthy classes, \textit{generative} and \textit{discriminative}. The first comprises of a  probability model for the pose variability of the objects together with an appearance model:
a probability model for the image appearance conditional on a given pose, together with a model for
background, i.e. non-object images. The model parameters can be estimated from training data and the
decisions are based on ratios of posterior probabilities. The second typically builds a classifier that can
discriminate between images (or sub-images) containing the object and those not containing the object.
The parameters of the classifier are carefully picked to reduce errors on the training data, often with a
regularization bias to avoid \textit{ overfitting}. Other differences among detection algorithms have to do with
the computational mechanism used to search the entire image or scan over possible poses, the type of image
representation with which the models are constructed, and what type and how much training data is
required to build a model.


\subsection{
Semantic Segmentation
}
Segmentation is fundamental for image analysis tasks. Semantic segmentation portrays the process of associating
each pixel of an image with a class label,  (for example, bloom, individual, street, sky, sea, or vehicle). 

Semantic image segmentation can be connected successfully to any task that involves the segmentation of of visual data. Examples include road segmentation for autonomous vehicles, medical image segmentation, scene segmentation for robot perception, and in image editing tools. While at present, accessible frameworks give precise object recognition, they are unfit to depict the limits between objects with the same accuracy. 

Oxford scientists have built up a novel neural network component  for a semantic segmentation that improves the capacity to perceive and depict objects. This development can be applied to improve any circumstance requiring the segmentation of visual information. 

Semantic image segmentation assumes a vital job in image understanding, enabling a computer to perceive objects in images. Recognition and depiction of objects are accomplished through the classification of every pixel in a image. Such procedures have a wide scope of utilization in computer vision, in diverse and developing fields, for example, vehicle autonomy and medical imaging. 

The previous state-of-the-art image segmentation frameworks utilized \textit{Fully Convolutional Neural Network (FCNN)} components, which offer magnificent exactness in detecting objects. While this advancement displayed a huge improvement in the semantic segmentation, these systems don't perform well in depicting object boundaries. \textit{Conditional Random Fields  (CRFs)} can be utilized in a post-processing
step to improve object boundary depiction, be that as it may, this isn't an ideal solution inferable from an absence of integration with the deep network. Figure \ref{fig:semantic} clearly shows the semantic segmentation of an image with classes like window, door, car, road, pavement, etc.



\begin{figure}[H]
  \centering
  \includegraphics[height=3in]{images/semantic.png}
   \caption{Image with semantic segmentation. Source:\cite{DBLP:journals/corr/abs-1809-10198}}
   \label{fig:semantic}
\end{figure}
Oxford researchers have built up a neural network component for semantic segmentation that bridles the outstanding object recognition of FCNNs and the incredible boundary delineation of CRFs. CRFs are fully integrated as recurrent neural networks, bringing about a framework that enhanced performance compared to the previous state-of-the-art. The novel system can be utilized to any task that includes the segmentation of visual data. Examples include road segmentation for autonomous vehicles, medical image segmentation, scene segmentation for robot perception, and in image editing tools. Oxford University Innovation is looking for modern industrial accomplices that desire to investigate the utilization of this framework for business applications.

\subsection{
Instance Segmentation
}
Instance segmentation is one step ahead of semantic segmentation wherein alongside pixel level classification, we anticipate that the computer should characterize each occurrence of a class independently.  The semantic segmentation does not separate between the examples of a specific class but instance segmentation does. Instance segmentation differentiate between the instances of a particular class. Figure \ref{fig:instance} below show instances of persons. Each person was identified distinctly and recognized as a separate entity.


\begin{figure}[H]
  \centering
  \includegraphics[height=3in]{images/instance.png}
   \caption{Image with instance segmentation.Source:\cite{L}}
   \label{fig:instance}
\end{figure}

\section{CNN for Object Detection and  Segmentation }
\subsection{Convolutional Neural Networks (CNN)}
Convolutional neural networks (CNN) are neural networks. CNNs have weights, biases, and outputs through a nonlinear activation.
Other normal or regular neural networks receives inputs and the neurons fully connected to the next layers.
Neurons within the same layer don't share any connections. If regular neural
networks are utilized for images, they will be enormous in size because of the large number of neurons,
resulting in \textit{overfitting}.This cannot be utilized for images, as images are enormous in size. Images will increase
the model size as it requires a huge number of neurons. An image can be considered a
volume with dimensions of height, width, and depth \cite{bib:dlforcv}. Depth is the channel of an image,
which is red, blue, and green. The neurons of a CNN are arranged in a volumetric fashion
to maximize the volume. Each of the layers transforms the input volume to an
output volume as shown in the following image:
\begin{figure}[H]
	\centering
	\includegraphics[height=2in]{images/cnn.png}
	\caption{Structure of Convolutional Neural Networks (CNN).Source:\cite{bib:dlforcv}}
	\label{fig:cnn}
\end{figure}

Convolution neural network filters encode by transformation \cite{bib:dlforcv}. The learned filters detects and find 
features or patterns in images. The deeper the layer, the more abstract the pattern is. Some
analyses have shown that these layers have the ability to detect edges, corners, and
patterns. The learnable parameters in CNN layers are less than the dense layer.
\subsubsection{Kernel}
Kernel is the parameter convolution layer used to convolve the image. It is the core part of the CNN. The convolution
operation is shown in the following figure:

\begin{figure}[H]
	\centering
	\includegraphics[height=2in]{images/kernel.png}
	\caption{The kernel of a Convolutional Neural Networks (CNN). Source:\cite{bib:dlforcv}}
	\label{fig:kernel}
\end{figure}
The kernel has two parameters, called \textit{stride} and \textit{size}. The size can be any dimension of a
rectangle. Stride is the number of pixels moved every time. A stride of length 1 produces an
image of almost the same size, and a stride of length 2 produces half the size. Padding the
image will help in achieving the same size of the input\cite{bib:dlforcv}.

\subsubsection{Max pooling}
Pooling layers are placed between convolution layers. Pooling layers reduce the size of the
image across layers by sampling. The sampling is done by selecting the maximum value in
a window. Average pooling averages over the window. Pooling also acts as a \textit{regularization}
technique to avoid \textit{overfitting}. Pooling is carried out on all the channels of features. Pooling
can also be performed with various strides\cite{bib:dlforcv}.
The size of the window is a measure of the receptive field of CNN. The following figure
shows an example of max pooling:
\begin{figure}[H]
	\centering
	\includegraphics[height=2.5in]{images/maxpooling.png}
	\caption{The Max Pooling of a Convolutional Neural Networks (CNN). Source:\cite{bib:dlforcv}}
	\label{fig:maxpool}
\end{figure}
CNN is the single most crucial component of any deep learning model for computer
vision. It is widely applied today in images and video.Perhaps, it won't be an exaggeration to state that it will be impossible for any computer to have vision without a CNN.

\subsection{Mask R-CNN}
Mask RCNN is a profound neural system meant to take care of occurrence division issue in machine learning or computer vision. As it were, it can isolate various items in a picture or a video. You give it a picture, it gives you the object bounding boxes, classes and masks. 

There are two phases of Mask RCNN. Initially, it produces proposals about the areas where there may be an object, dependent on the input picture. Second, it predicts the class of the object, refines the bounding box and produces a masks in pixel dimension of the object dependent on the first stage proposition. We will describe Mask R-CNN fully below. We have four components of Mask R-CNN viz; The backbone, Regional Proposal Network,{ROI Classifier and Bounding Box Regressor,Segmentation mask. These components will be discussed in detail in the sections below.
\subsubsection{Backbone}
\paragraph{Residual Networks (RESNET)}

ResNet is an essential neural network that serves as a backbone to Mask R-CNN and numerous computer vision tasks. 
It makes the training of extremely deep neural networks possible which was very difficult before then due to the 
challenge of vanishing gradients, that hampers convergence in the network. According to \protect\cite{M} before RESNET, the problem 
of \textit{vanishing gradient} has been mainly addressed by normalized initialization \protect\cite{N},\protect\cite{GlorotBengio2010}, \protect\cite{P}, \protect\cite{Q} and intermediate 
normalization layers \protect\cite{R}, which enable networks with tens of layers to start converging for stochastic gradient descent 
(SGD) with backpropagation \protect\cite{S}.
Looking at the sample scenario of the vanishing gradients or degradation. A worst-case scenario of vanishing gradient is
 the case was the early layers of a deeper model can be replaced with a shallow network and the other layers can act as an identity function.  The shallow network and its deeper counterpart give the same accuracy. So deeper models do not perform well due to degradation. When a deeper network is used it approximates the mapping than its shallower variant and decreases the error by a notable margin. 
 Also, the deeper network had issues of degradation.
To solve this problem ResNet introduced the concept of skip connection.

\begin{figure}[H]
 \centering
 \includegraphics[height=2.3in]{images/skipconnection.jpg}
 \caption{Skip connection of ResNet.Source:\cite{M}}
\end{figure}

Without a skip connection, deep convolution networks are stacked together one after the other.  With a skip connection deep convolution networks are tacked together but this time the original input is added to the output of the convolution block.
Mathematically representing this, we can consider a mapping or space G(x) to be fitted by some stacked layers of an entire network.  X denotes the inputs in the first layer of the net. This layer will approximate a residual function Z(x) = G(x)-x by hypothesizing.  Therefore the original function or mapping G(x) becomes Z(x) + x.  The input and output dimensions are expected to be of the same dimension for this work properly.  It is worth noting that ResNet, contained 152 layers, won ILSVRC 2015 with an error rate of 3.6 percent beating even humans with their error rate of circa 5 – 10 percent, and replacing VGG-16 layers in Faster R-CNN with ResNet-101 produced relative improvements of 28 percent. It also trained networks with 100 layers and 1000 layers\cite{M} . 

\begin{figure}[H]
  \includegraphics[height=2.2in]{images/resnet-graph.jpg}
   \caption{Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts, see \cite{M}.}
\end{figure}

\paragraph{Feature Pyramid Network}

Feature Pyramid Network (FPN) is a generic feature extractor used in various application for recognizing objects at different scales, developed by \cite{K}. For the recognition system for detecting objects at various scales, feature pyramid is a primary constituent of such a system. Pyramid representation has the problem of computing and memory intensiveness and has been avoided in the deep learning object detectors. 
Feature Pyramid Network (FPN) solves this problem by restructuring the architecture of the pyramid to a top-down architecture with lateral connections. The multi-scale, pyramidal hierarchy of deep ConvNet was leveraged to develop Feature Pyramid Network (FPN). Pyramids of  FPN are scale-invariant. This means that when an object scale changes it is offset by shifting its level in the pyramid.  
 Before the introduction of FPN, some ways were used in the extraction of features from images. Initially, hand-engineered features \cite{Lowe2004} were used and it makes use of featurized image pyramids. ConvNet is more robust to variance in scale, capable of representing higher-level semantics, and so features from it have quickly replaced engineered features. 
 According to \cite{L} this ConvNets gives multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.  Featuring each level of an image pyramid comes with the profound limitation of increase in inference time which makes it impractical for real applications. FPN explored the pyramidal shape of a ConvNet’s feature hierarchy to build a feature pyramid that has strong features
  with high-resolution at all scales. 
 This gives a feature pyramid that has profound semantics at all phases and is constructed quickly  from a unit  input image scale 
\begin{figure}[H]
\centering
  \includegraphics[height=3in]{images/fpn.jpg}
   \caption{Top: a top-down architecture with skip connections, where predictions are made on the finest level (e.g., \protect\cite{T}). Bottom: FPN model that has a similar structure but leverages it as a feature pyramid, with predictions made independently at all levels \protect\cite{K}.}
\end{figure}

FPN was applied in Regional Proposal Network (RPN) and Fast R-CNN. With the new adaptations, RPN could be naturally trained and tested with FPN, Using FPN in a basic Faster R-CNN system, the result surpasses all existing single-model entries including those from the COCO 2016 challenge winners.

\subsubsection{Regional Proposal Network}

Regional Proposal Network (RPN) is a useful network effectively used in R-CNN that scans the image in a sliding window pattern over the anchors.  It proposes multiple objects that are recognizable in a particular image. The last convolutional layer that is produced by the Faster R-CNN is called the feature map. A proposal is generated for the region where the object lies by sliding over a feature map a network, which is the RPN. RPN suggest where an object lies in an image.
\begin{figure}[H]
\centering
  \includegraphics[height=3in]{images/rcnn-rpn.jpg}
   \caption{The architecture of Faster R-CNN. RPN generate the proposal for the objects. \protect\cite{J}.}
\end{figure}

\begin{figure}[H]
\centering
  \includegraphics[height=3in]{images/rpn-arch.jpg}
   \caption{RPN Architecture \protect\cite{J}.}
\end{figure}

Analyzing the architecture of RPN, the intermediate layer divides into a \textit{classifier} and \textit{regressor} layers, and the 
concept of the anchor was introduced. Anchor are boxes of different sizes and aspect ratio that are generated over an 
image that determines the ideal location, shape, and size of objects in the image. They overlap to fill up as much of 
the image as possible. Thousands of anchor boxes are generated for this. For each anchor box, the object’s bounding box 
that has the highest overlap is divided by non-overlap. This is termed \textit{Intersection Over Union (IOU)}. If the highest IOU 
is greater than 50 percent, the anchor box determines the object that gave the highest IOU. But if it is greater than 40 
percent the true detection is ambiguous, and if it is less than 40 percent it predicts no object. Classifier gives the
 probability of a proposal containing the target object. Regression regresses the coordinates of the proposals. RPN is 
 also used in Mask RCNN.
\begin{figure}[H]
  \centering
  \includegraphics[height=3in]{images/rpn1.png}
  
  \caption{Anchor boxes (dotted) and the shift/scale applied to them to fit the object precisely (solid).
  Several anchors can map to the same object. Source: \textbf{\textit{Author}}}
  
\end{figure}

The RPN produces two results for each anchor; Anchor Class i.e. either the foreground or the background, and
 Bounding Box Refinement, an estimation to rectify the anchor box that fits the object well. This is a change in x,y,
  width, height.

\subsubsection{ROI Classifier and Bounding Box Regressor}
Region of Interest (ROI) classifier is proposed by the Region Proposal Network (RPN) and similarly,
 like the RPN, it produces two results for each ROI. First, the \textit{class}, it produces the classes
  of the object in the ROI, but it is deeper and can classify regions to specific classes (car, person, nucleus, etc). 
  And secondly the\textit{ Bounding Box Refinement}, which further refines the location and size of the bounding box to envelope the object.

\paragraph{ROI Pooling}

Input sizes of a classifier vary. Classifiers require fixed, stable input size and can’t manage varying input sizes. 
Bounding box refinement in RPN produces ROI boxes of various sizes. ROI Pooling tackles this challenge.

Cropping a part of a feature map and resizing it to a fixed size is termed ROI pooling. It is very much alike to the concept of resizing a cropped image.
 
\begin{figure}[H] 
\centering
  \pgfimage[height=1.8in]{images/roi1.png}
   \caption{ROI Pooling. Source: \textbf{\textit{Author}}}
\end{figure}

\subsubsection{Segmentation Mask}
Mask RCNN further added an extra branch to what Faster RCNN has. This is the mask branch. The Mask branch is a convolutional network that receives the positive regions selected by the ROI classifier and builds low resolution masks for them. The resolution is about 28x 28 pixels. They are soft masks, constituted by float numbers, so they contain more ingredients than binary masks. The size of the mask enables the mask branch to be light. When training the model, the ground-truth masks is scaled down to 28x28 to compute the loss, and when inferencing the predicted masks is scaled up to the size of the ROI bounding box and that produces the final masks, one per object\cite{Y}.

\begin{figure}[H]
  \centering
    \includegraphics[height=1.7in]{images/mask.png}
     \caption{Segmentation Mask of a drone-based imageset . Source: \textbf{\textit{Author}} }
  \end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[height=1.7in]{images/maskmodel.png}
	\caption{Different Mask R-CNN head architectures schema, . Source:\cite{L} }
	\label{fig:head}
\end{figure}

\section{Drone-Based Dataset}

Drones (or UAVs) furnished with high resolution cameras have been utilized in a wide range of applications, 
including agricultural, aerial photography, fast delivery, surveillance, etc. This has made, automatic comprehension of 
visual data collected from drones become highly demanding, which brings computer vision to drones more and more closely.
 Outstanding advancements have been made in general computer vision algorithms, such as detection and tracking, yet these algorithms 
 are not usually flawless for dealing with sequences or images captured by drones, due to various difficulties such as view point changes 
 and scales. Consequently, developing and appraising new vision algorithms for drone generated visual data is a key problem in drone-based 
 applications.
The major challenge of segmentation in drone generated visual data is the lack of proper datasets for these. 
A VisDrone Dataset was produced in \protect\cite{V}. The images and video sequences in the benchmark were captured over
 various urban/suburban areas of 14 different cities across China from north to south. Specifically, VisDrone2018 
 consists of 263 video clips and 10; 209 images (no overlap with video clips) with rich annotations, including object 
 bounding boxes, object categories, occlusion, truncation ratios, etc. With intensive amount of effort, our benchmark 
  has more than 2:5 million annotated instances in 179; 264 images/video frames \protect\cite{V}. Also Semantic Drone Dataset that focuses on
   semantic understanding of urban scenes for increasing the safety of autonomous drone flight and landing procedures. The imagery depicts 
   more than 20 houses from nadir (bird's eye) view acquired at an altitude of 5 to 30 meters above ground. A high resolution camera was 
   used to acquire images at a size of 6000x4000px (24Mpx). The training set contains 400 publicly available images and the test set is 
    made up of 200 private images.\footnote{http://dronedataset.icg.tugraz.at}.

\begin{figure}[H]
  \centerline{%
    \includegraphics[height=2.2in]{images/045.jpg}%
  }
 
  \caption{Drone-based imageset 1}
  \label{image_1}
\end{figure}

%
  
\begin{figure}[H]
  \centering
  \includegraphics[height=2.2in]{images/044.jpg}
  
  \caption{Drone-based imageset 2}
  \label{image 2}
\end{figure}